
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnBr6sqZy49h/GS88Z5NGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FinAminToastCrunch/Probabalistic_Graph_Residual/blob/main/notebook_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-geometric\n",
        "!pip install deeprobust"
      ],
      "metadata": {
        "id": "63b7jO97FcE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  dataset = 'Cora'\n",
        "  seed = 15\n",
        "  model = 'AirGNN'\n",
        "  alpha = 0.1\n",
        "  lambda_amp = 0.1\n",
        "  lcc = False\n",
        "  normalize_features = True\n",
        "  random_splits = False\n",
        "  runs = 10\n",
        "  epochs = 10\n",
        "  lr = 0.01\n",
        "  weight_decay = 0.0005\n",
        "  early_stopping = 100\n",
        "  hidden = 64\n",
        "  dropout = 0.8\n",
        "  K = 10\n",
        "  model_cache = False"
      ],
      "metadata": {
        "id": "UH5uPkcNF-Jx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kBsHhz7FQxY",
        "outputId": "f0932ba9-0733-481d-deec-bf8d260f4251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "#adv_attack.py\n",
        "\n",
        "from deeprobust.graph.data import Dataset\n",
        "from deeprobust.graph.defense import GCN\n",
        "from deeprobust.graph.targeted_attack import Nettack\n",
        "import pickle\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "#from datasets import uniqueId, str2bool\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device:\", device)\n",
        "\n",
        "def cache_nettack_feature_attack_data(args):\n",
        "    n_perturbations_candidates = [0, 1, 2, 5, 10, 20, 50, 80]\n",
        "    print(\"====preparing dataset: %s=====\" % (args.dataset))\n",
        "    cache_dataset(args.dataset, n_perturbations_candidates, args)\n",
        "\n",
        "\n",
        "def cache_dataset(dataset_name, n_perturbations_candidates, args):\n",
        "    data = Dataset(root='/tmp/', name=(dataset_name).lower(), seed=args.seed)\n",
        "    adj, features, labels = data.adj, data.features, data.labels\n",
        "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "    surrogate = GCN(nfeat=features.shape[1], nclass=labels.max().item() + 1,\n",
        "                    nhid=16, dropout=0, with_relu=False, with_bias=False, device=device)\n",
        "    surrogate = surrogate.to(device)\n",
        "    surrogate.fit(features, adj, labels, idx_train)\n",
        "\n",
        "    node_list = select_nodes(data)\n",
        "    print(data, node_list, args)\n",
        "\n",
        "    for target_node in tqdm(node_list):\n",
        "        for perturbation in n_perturbations_candidates:\n",
        "\n",
        "            uid = uniqueId(dataset_name, target_node, perturbation)\n",
        "            n_perturbations = int(perturbation)\n",
        "            if n_perturbations != 0:\n",
        "                model = Nettack(surrogate, nnodes=adj.shape[0], attack_structure=False,\n",
        "                                attack_features=True, device=device)\n",
        "                model = model.to(device)\n",
        "                model.attack(features, adj, labels, target_node, n_perturbations, verbose=False)\n",
        "                modified_features = model.modified_features\n",
        "                data.features = modified_features\n",
        "\n",
        "            pickle.dump(data, open(\"./fixed_data/adv_attack/\" + uid + \".pickle\", 'wb'))\n",
        "            print(uid, \"has been save\")\n",
        "\n",
        "def classification_margin(output, true_label):\n",
        "    \"\"\"Calculate classification margin for outputs.\n",
        "    `probs_true_label - probs_best_second_class`\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.Tensor\n",
        "        output vector (1 dimension)\n",
        "    true_label: int\n",
        "        true label for this node\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        classification margin for this node\n",
        "    \"\"\"\n",
        "    probs = torch.exp(output)\n",
        "    probs_true_label = probs[true_label].clone()\n",
        "    probs[true_label] = 0\n",
        "    probs_best_second_class = probs[probs.argmax()]\n",
        "    return (probs_true_label - probs_best_second_class).item()\n",
        "\n",
        "\n",
        "def select_nodes(data, target_gcn=None):\n",
        "    '''\n",
        "    selecting nodes as reported in nettack paper:\n",
        "    (i) the 10 nodes with highest margin of classification, i.e. they are clearly correctly classified,\n",
        "    (ii) the 10 nodes with lowest margin (but still correctly classified) and\n",
        "    (iii) 20 more nodes randomly\n",
        "    '''\n",
        "    adj, features, labels = data.adj, data.features, data.labels\n",
        "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "\n",
        "    if target_gcn is None:\n",
        "        target_gcn = GCN(nfeat=features.shape[1],\n",
        "                         nhid=16,\n",
        "                         nclass=labels.max().item() + 1,\n",
        "                         dropout=0.5, device=device)\n",
        "        target_gcn = target_gcn.to(device)\n",
        "        target_gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
        "    target_gcn.eval()\n",
        "    output = target_gcn.predict()\n",
        "\n",
        "    margin_dict = {}\n",
        "    for idx in idx_test:\n",
        "        margin = classification_margin(output[idx], labels[idx])\n",
        "        if margin < 0:  # only keep the nodes correctly classified\n",
        "            continue\n",
        "        margin_dict[idx] = margin\n",
        "    sorted_margins = sorted(margin_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    high = [x for x, y in sorted_margins[: 10]]\n",
        "    low = [x for x, y in sorted_margins[-10:]]\n",
        "    other = [x for x, y in sorted_margins[10: -10]]\n",
        "    other = np.random.choice(other, 20, replace=False).tolist()\n",
        "\n",
        "    return high + low + other\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     # parser = argparse.ArgumentParser()\n",
        "#     # parser.add_argument('--dataset', type=str, required=True)\n",
        "#     # parser.add_argument('--seed', type=int, default=15)\n",
        "#     # args = parser.parse_args()\n",
        "#     args = Args()\n",
        "#     print('arg : ', args)\n",
        "#     cache_nettack_feature_attack_data(args)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.py\n",
        "\n",
        "import os.path as osp\n",
        "import torch\n",
        "\n",
        "from torch_geometric.datasets import Planetoid, Coauthor, Amazon\n",
        "import torch_geometric.transforms as T\n",
        "from deeprobust.graph.data import Dataset, Dpr2Pyg\n",
        "\n",
        "import argparse\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Unsupported value encountered.')\n",
        "\n",
        "def uniqueId(dataset_name, target_node, perturbation):\n",
        "    return dataset_name.lower() +\"_\"+str(target_node)+\"_\"+str(perturbation)\n",
        "\n",
        "\n",
        "def prepare_data(args, lcc=False):\n",
        "    transform = T.ToSparseTensor()\n",
        "    if args.dataset == \"Cora\" or args.dataset == \"CiteSeer\" or args.dataset == \"PubMed\":\n",
        "        if lcc:\n",
        "            dpr_data = Dataset(root='/tmp/', name=(args.dataset).lower())\n",
        "            dataset = Dpr2Pyg(dpr_data, transform=transform)\n",
        "        else:\n",
        "            dataset = get_planetoid_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_planetoid_splits if args.random_splits else None\n",
        "\n",
        "    elif args.dataset == \"cs\" or args.dataset == \"physics\":\n",
        "        dataset = get_coauthor_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_coauthor_amazon_splits\n",
        "\n",
        "    elif args.dataset == \"computers\" or args.dataset == \"photo\":\n",
        "        dataset = get_amazon_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_coauthor_amazon_splits\n",
        "    print(\"Data:\", dataset[0])\n",
        "\n",
        "    return dataset, permute_masks\n",
        "\n",
        "\n",
        "def get_planetoid_dataset(name, normalize_features=False, transform=None):\n",
        "    #path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    #print(f\"path = {path}\")\n",
        "    path = '/content/../data/Cora'\n",
        "    dataset = Planetoid(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coauthor_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    dataset = Coauthor(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_amazon_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    dataset = Amazon(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def index_to_mask(index, size):\n",
        "    mask = torch.zeros(size, dtype=torch.bool, device=index.device)\n",
        "    mask[index] = 1\n",
        "    return mask\n",
        "\n",
        "\n",
        "def random_planetoid_splits(data, num_classes, lcc_mask=None):\n",
        "    # Set new random planetoid splits:\n",
        "    # * 20 * num_classes labels for training\n",
        "    # * 500 labels for validation\n",
        "    # * 1000 labels for testing\n",
        "\n",
        "    indices = []\n",
        "    if lcc_mask is not None:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y[lcc_mask] == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0))]\n",
        "            indices.append(index)\n",
        "    else:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0))]\n",
        "            indices.append(index)\n",
        "\n",
        "    train_index = torch.cat([i[:20] for i in indices], dim=0)\n",
        "\n",
        "    rest_index = torch.cat([i[20:] for i in indices], dim=0)\n",
        "    rest_index = rest_index[torch.randperm(rest_index.size(0))]\n",
        "\n",
        "    data.train_mask = index_to_mask(train_index, size=data.num_nodes)\n",
        "    data.val_mask = index_to_mask(rest_index[:500], size=data.num_nodes)\n",
        "    data.test_mask = index_to_mask(rest_index[500:1500], size=data.num_nodes)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def random_coauthor_amazon_splits(data, num_classes, lcc_mask=None, seed=None):\n",
        "    # Set random coauthor/co-purchase splits:\n",
        "    # * 20 * num_classes labels for training\n",
        "    # * 30 * num_classes labels for validation\n",
        "    # rest labels for testing\n",
        "    g = None\n",
        "    if seed is not None:\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "\n",
        "    indices = []\n",
        "    if lcc_mask is not None:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y[lcc_mask] == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0), generator=g)]\n",
        "            indices.append(index)\n",
        "    else:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0), generator=g)]\n",
        "            indices.append(index)\n",
        "\n",
        "    train_index = torch.cat([i[:20] for i in indices], dim=0)\n",
        "    val_index = torch.cat([i[20:50] for i in indices], dim=0)\n",
        "\n",
        "    rest_index = torch.cat([i[50:] for i in indices], dim=0)\n",
        "    rest_index = rest_index[torch.randperm(rest_index.size(0))]\n",
        "\n",
        "    data.train_mask = index_to_mask(train_index, size=data.num_nodes)\n",
        "    data.val_mask = index_to_mask(val_index, size=data.num_nodes)\n",
        "    data.test_mask = index_to_mask(rest_index, size=data.num_nodes)\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "SjTLKod8FZIX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from typing import Optional, Tuple\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "\n",
        "class AirGNN(torch.nn.Module):\n",
        "    def __init__(self, dataset, args):\n",
        "        super(AirGNN, self).__init__()\n",
        "        self.dropout = args.dropout\n",
        "        self.lin1 = Linear(dataset.num_features, args.hidden)\n",
        "        self.lin2 = Linear(args.hidden, dataset.num_classes)\n",
        "        self.prop = AdaptiveMessagePassing(K=args.K, alpha=args.alpha, mode=args.model, args=args)\n",
        "        print(self.prop)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "        self.prop.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.adj_t, data.y\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        x = self.prop(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class AdaptiveMessagePassing(MessagePassing):\n",
        "    _cached_edge_index: Optional[Tuple[Tensor, Tensor]]\n",
        "    _cached_adj_t: Optional[SparseTensor]\n",
        "\n",
        "    def __init__(self,\n",
        "                 K: int,\n",
        "                 alpha: float,\n",
        "                 dropout: float = 0.,\n",
        "                 cached: bool = False,\n",
        "                 add_self_loops: bool = True,\n",
        "                 normalize: bool = True,\n",
        "                 mode: str = None,\n",
        "                 node_num: int = None,\n",
        "                 args=None,\n",
        "                 **kwargs):\n",
        "\n",
        "        super(AdaptiveMessagePassing, self).__init__(aggr='add', **kwargs)\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "        self.mode = mode\n",
        "        self.dropout = dropout\n",
        "        self.cached = cached\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.normalize = normalize\n",
        "        self._cached_edge_index = None\n",
        "        self.node_num = node_num\n",
        "        self.args = args\n",
        "        self._cached_adj_t = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self._cached_edge_index = None\n",
        "        self._cached_adj_t = None\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, edge_weight: OptTensor = None, mode=None) -> Tensor:\n",
        "        if self.normalize:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                raise ValueError('Only support SparseTensor now')\n",
        "\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                cache = self._cached_adj_t\n",
        "                if cache is None:\n",
        "                    edge_index = gcn_norm(  # yapf: disable\n",
        "                        edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                        add_self_loops=self.add_self_loops, dtype=x.dtype)\n",
        "                    if self.cached:\n",
        "                        self._cached_adj_t = edge_index\n",
        "                else:\n",
        "                    edge_index = cache\n",
        "\n",
        "        if mode == None: mode = self.mode\n",
        "\n",
        "        if self.K <= 0:\n",
        "            return x\n",
        "        hh = x\n",
        "\n",
        "        if mode == 'MLP':\n",
        "            return x\n",
        "\n",
        "        elif mode == 'APPNP':\n",
        "            x = self.appnp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K, alpha=self.alpha)\n",
        "\n",
        "        elif mode in ['AirGNN']:\n",
        "            x = self.amp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K)\n",
        "        else:\n",
        "            raise ValueError('wrong propagate mode')\n",
        "        return x\n",
        "\n",
        "    def appnp_forward(self, x, hh, edge_index, K, alpha):\n",
        "        for k in range(K):\n",
        "            x = self.propagate(edge_index, x=x, edge_weight=None, size=None)\n",
        "            x = x * (1 - alpha)\n",
        "            x += alpha * hh\n",
        "        return x\n",
        "\n",
        "\n",
        "    def compute_fast_KL(self, m1, s1, p1, m2, s2, p2):\n",
        "        '''Compute the KLDivergence between two gaussians, KL(P|Q) scaled between 0 and 1'''\n",
        "        #note: precision = p = inv(s)\n",
        "        d = len(m1)\n",
        "        kl = 0.5*( np.log2( np.linalg.det(s2)/np.linalg.det(s1) ) - d + np.trace(np.matmul(p2, s1)) + ((m2-m1).T)@(p2)@(m2-m1) )\n",
        "        scaled_kl = 1 - np.exp(-kl)\n",
        "\n",
        "        return scaled_kl\n",
        "\n",
        "    def amp_forward(self, x, hh, K, edge_index):\n",
        "        lambda_amp = self.args.lambda_amp\n",
        "        gamma = 1 / (2 * (1 - lambda_amp))  ## or simply gamma = 1\n",
        "\n",
        "        for k in range(K):\n",
        "            y = x - gamma * 2 * (1 - lambda_amp) * self.compute_LX(x=x, edge_index=edge_index)  # Equation (9)\n",
        "            gmXin = GaussianMixture(n_components=1, random_state=0).fit(hh.detach().to('cpu').numpy())\n",
        "            gmY = GaussianMixture(n_components=1, random_state=0).fit(y.detach().to('cpu').numpy())\n",
        "            scaled_KL = self.compute_fast_KL(m1=gmXin.means_[0], s1=gmXin.covariances_[0], p1 = gmXin.precisions_[0],\n",
        "                                            m2=gmY.means_[0], s2 = gmY.covariances_[0], p2 = gmY.precisions_[0])\n",
        "            #x = hh + (1-scaled_KL)*(y-hh) #we use 1-kl because we want to more heavily weigh closer samples.\n",
        "            x = hh + (scaled_KL)*(y-hh) #we use 1-kl because we want to more heavily weigh closer samples.\n",
        "\n",
        "            #x = hh + self.proximal_L21(x=y - hh, lambda_=gamma * lambda_amp) # Equation (11) and (12)\n",
        "        return x\n",
        "\n",
        "    def proximal_L21(self, x: Tensor, lambda_):\n",
        "        row_norm = torch.norm(x, p=2, dim=1)\n",
        "        score = torch.clamp(row_norm - lambda_, min=0)\n",
        "        index = torch.where(row_norm > 0)             #  Deal with the case when the row_norm is 0\n",
        "        score[index] = score[index] / row_norm[index] # score is the adaptive score in Equation (14)\n",
        "        return score.unsqueeze(1) * x\n",
        "\n",
        "    def compute_LX(self, x, edge_index, edge_weight=None):\n",
        "        x = x - self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(K={}, alpha={}, mode={}, dropout={}, lambda_amp={})'.format(self.__class__.__name__, self.K,\n",
        "                                                               self.alpha, self.mode, self.dropout,\n",
        "                                                               self.args.lambda_amp)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vdO8owV9FoLK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "#from datasets import prepare_data\n",
        "#from model import AirGNN\n",
        "from tqdm import tqdm\n",
        "#from train_eval import evaluate\n",
        "import numpy as np\n",
        "from torch import tensor\n",
        "#from datasets import uniqueId, str2bool\n",
        "import torch_geometric.transforms as T\n",
        "from deeprobust.graph.data import Dpr2Pyg\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='Cora')\n",
        "# parser.add_argument('--model', type=str, default='AirGNN')\n",
        "# parser.add_argument('--normalize_features', type=str2bool, default=True)\n",
        "# parser.add_argument('--random_splits', type=str2bool, default=False)\n",
        "# parser.add_argument('--runs', type=int, default=10)\n",
        "# parser.add_argument('--dropout', type=float, default=0.5, help=\"dropout\")\n",
        "# parser.add_argument('--hidden', type=int, default=64)\n",
        "# parser.add_argument('--K', type=int, default=10, help=\"the number of propagagtion in AirGNN\")\n",
        "# parser.add_argument('--alpha', type=float, default=None)\n",
        "# parser.add_argument('--lambda_amp', type=float, default=0.1)\n",
        "\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = Args()\n",
        "\n",
        "print('arg : ', args)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device: in train_eval:\", device)\n",
        "\n",
        "\n",
        "def main():\n",
        "    acc_lst_dic = {}\n",
        "    final_result = {}\n",
        "    n_perturbations_candidates = [0, 1, 2, 5, 10, 20, 50, 80]\n",
        "    for perturbation_number in n_perturbations_candidates:\n",
        "        acc_lst_dic[perturbation_number] = []\n",
        "\n",
        "    if args.dataset in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
        "        for run_k in range(args.runs):\n",
        "            perturbation_acc_dic = nettack_run(args.dataset.lower(), run_k, n_perturbations_candidates)\n",
        "            for key, val in perturbation_acc_dic.items():\n",
        "                acc_lst_dic[key].append(val)\n",
        "\n",
        "        for key, val in acc_lst_dic.items():\n",
        "            acc_lst = tensor(val)\n",
        "            final_result[key] = '{:.3f} ± {:.3f}'.format(acc_lst.mean().item(), acc_lst.std().item())\n",
        "            \n",
        "        print(\"Dataset:{}, model:{}\".format(args.dataset, args.model))\n",
        "        print(\"Average performance on 40 targeted nodes with 10 runs:\", final_result)\n",
        "\n",
        "def nettack_run(dataset_name, run_k, n_perturbations_candidates):\n",
        "    node_list = get_target_nodelst(dataset_name)\n",
        "    num = len(node_list)\n",
        "    assert num == 40\n",
        "\n",
        "    target_accuracy_dic = {}\n",
        "    target_accuracy_summary_dic = {}\n",
        "    for key in n_perturbations_candidates:\n",
        "        target_accuracy_dic[key] = 0\n",
        "        target_accuracy_summary_dic[key] = []\n",
        "\n",
        "    for target_node in node_list:\n",
        "        for perturbation in n_perturbations_candidates:\n",
        "            n_perturbations = int(perturbation)\n",
        "            uid = uniqueId(dataset_name, target_node, perturbation)\n",
        "            data = get_adv_data(uid)\n",
        "            target_node_acc = adv_test(target_node, data, data.adj[target_node].nonzero()[1].tolist(), run_k)\n",
        "            if target_node_acc == 0:\n",
        "                target_accuracy_dic[perturbation] += 1\n",
        "\n",
        "            print(\"=========Attacked Node: {:d}, n_perturbations: {:.2f}=========\".format(target_node, perturbation))\n",
        "        print(args.model, args.lambda_amp)\n",
        "\n",
        "    assert num == 40\n",
        "    for key in target_accuracy_dic.keys():\n",
        "        target_accuracy_dic[key] = 1 - target_accuracy_dic[key] / num\n",
        "\n",
        "    print(\"Accuracy on 40 target nodes:\", target_accuracy_dic)\n",
        "    return target_accuracy_dic\n",
        "\n",
        "\n",
        "def get_target_nodelst(dataset_name):\n",
        "    allfiles = os.listdir(\"./fixed_data/adv_attack\")\n",
        "    remain = [single_file for single_file in allfiles if dataset_name.lower() in single_file]\n",
        "    node_indexes = []\n",
        "    for i in remain:\n",
        "        if len(i.split(\"_\"))  == 3:\n",
        "            node_indexes.append(int(i.split(\"_\")[1]))\n",
        "\n",
        "    node_list = list(set(node_indexes))\n",
        "    assert len(node_list) == 40\n",
        "    return node_list\n",
        "\n",
        "def get_adv_data(uid):\n",
        "    print(\"./fixed_data/adv_attack/\"+uid+\".pickle\")\n",
        "    if os.path.isfile(\"./fixed_data/adv_attack/\"+uid+\".pickle\"):\n",
        "        return pickle.load(open(\"./fixed_data/adv_attack/\"+uid+\".pickle\",'rb'))\n",
        "    else:\n",
        "        raise Exception(\"ERROR\" + uid + \" file not found\")\n",
        "\n",
        "def adv_test(key_node_index, attacked_dpr_data, neighbor_lst, run_k):\n",
        "    transform = T.ToSparseTensor()\n",
        "    dataset = Dpr2Pyg(attacked_dpr_data, transform=transform)\n",
        "    data = dataset[0]\n",
        "    data = data.to(device)\n",
        "\n",
        "    if args.model in [\"APPNP\", \"AirGNN\", \"MLP\"]:\n",
        "        model = AirGNN(dataset, args)\n",
        "    else:\n",
        "        raise Exception(\"Unsupported model mode!!!\")\n",
        "\n",
        "    # 10 best models will be tested on the same attacked data\n",
        "    model.to(device).reset_parameters()\n",
        "    checkpointPath = \"./model/lcc/{}_{}_best_model_run_{}.pth\".format(args.dataset, args.model, run_k)\n",
        "    print(\"checkpointPath:\", checkpointPath)\n",
        "    checkpoint = torch.load(checkpointPath)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # test the prediction of targeted node\n",
        "    model.eval()\n",
        "    logits = model(data)\n",
        "    probs = torch.exp(logits[[key_node_index]])\n",
        "    target_node_acc = (logits.argmax(1)[key_node_index] == data.y[key_node_index]).item()  # True/False\n",
        "    # print(\"single_target_predict\\n:\", logits.argmax(1)[key_node_index].item(), data.y[key_node_index].item(), target_node_acc)\n",
        "    return target_node_acc\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqyrNrcBG81q",
        "outputId": "e77eaca4-33e1-4530-9719-fcfd15f127ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arg :  <__main__.Args object at 0x7efda5dfb650>\n",
            "device: in train_eval: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_eval.py\n",
        "from __future__ import division\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from torch_geometric.utils import *\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "\n",
        "def run(dataset, model, runs, epochs, lr, weight_decay, early_stopping,\n",
        "        permute_masks=None, logger=None, lcc=False, save_path=None, args=None, target_node=None):\n",
        "    val_losses, accs, durations = [], [], []\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"device: in train_eval:\", device)\n",
        "\n",
        "    data = dataset[0]\n",
        "    \n",
        "    pbar = tqdm(range(runs), unit='run')\n",
        "\n",
        "    for runs_num, _ in enumerate(pbar):\n",
        "        if permute_masks is not None:\n",
        "            data = permute_masks(data, dataset.num_classes, lcc_mask=None, seed=runs_num)\n",
        "        data = data.to(device)\n",
        "\n",
        "        model.to(device).reset_parameters()\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        test_acc = 0\n",
        "        val_loss_history = []\n",
        "        \n",
        "        if args.lcc:\n",
        "            path = (\"./model/lcc/{}_{}_best_model_run_{}.pth\".format(args.dataset, args.model, runs_num))\n",
        "        else:\n",
        "            path = (\"./model/full/{}_{}_best_model_run_{}.pth\".format(args.dataset, args.model, runs_num))\n",
        "        \n",
        "        for epoch in range(1, epochs + 1):\n",
        "            out = train(model, optimizer, data)\n",
        "            eval_info = evaluate(model, data)\n",
        "            eval_info['epoch'] = epoch\n",
        "\n",
        "            if logger is not None:\n",
        "                logger(eval_info)\n",
        "\n",
        "            if eval_info['val_loss'] < best_val_loss:\n",
        "                best_val_loss = eval_info['val_loss']\n",
        "                test_acc = eval_info['test_acc']\n",
        "\n",
        "                if args.model_cache:\n",
        "                    # print(\"*** Saving Checkpoint ***\")\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict()\n",
        "                    }, path)\n",
        "\n",
        "            val_loss_history.append(eval_info['val_loss'])\n",
        "            if early_stopping > 0 and epoch > epochs // 2:\n",
        "                tmp = tensor(val_loss_history[-(early_stopping + 1):-1])\n",
        "                if eval_info['val_loss'] > tmp.mean().item():\n",
        "                    break\n",
        "\n",
        "        # to print results of this run\n",
        "        if logger is not None:\n",
        "            logger.print_statistics(runs_num)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_end = time.perf_counter()\n",
        "\n",
        "        val_losses.append(best_val_loss)\n",
        "        accs.append(test_acc)\n",
        "        durations.append(t_end - t_start)\n",
        "\n",
        "    # to print best results of all runs\n",
        "    if logger is not None:\n",
        "        logger.print_statistics()\n",
        "\n",
        "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
        "\n",
        "    print('Val Loss: {:.4f}, Test Accuracy: {:.3f} ± {:.3f}, Duration: {:.3f}'.\n",
        "          format(loss.mean().item(),\n",
        "                 acc.mean().item(),\n",
        "                 acc.std().item(),\n",
        "                 duration.mean().item()))\n",
        "    return acc.mean().item()\n",
        "\n",
        "\n",
        "def train(model, optimizer, data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    if len(data.y.shape) == 1:\n",
        "        y = data.y\n",
        "    else:\n",
        "        y = data.y.squeeze(1) ## for ogb data\n",
        "\n",
        "    loss = F.nll_loss(out[data.train_mask], y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "    outs = {}\n",
        "\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        mask = data['{}_mask'.format(key)]\n",
        "        # print(\"number:\", key, len(mask), mask.sum().item())\n",
        "        # print(key, mask)\n",
        "        if len(data.y.shape) == 1:\n",
        "            y = data.y\n",
        "        else:\n",
        "            y = data.y.squeeze(1) ## for ogb data\n",
        "\n",
        "        loss = F.nll_loss(logits[mask], y[mask]).item()\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(y[mask]).sum().item() / mask.sum().item()\n",
        "\n",
        "        outs['{}_loss'.format(key)] = loss\n",
        "        outs['{}_acc'.format(key)] = acc\n",
        "    return outs\n"
      ],
      "metadata": {
        "id": "r5cu2jl8HrmQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import optuna\n",
        "import torch_geometric.transforms as T\n",
        "import argparse\n",
        "#from datasets import str2bool\n",
        "#from train_eval import run\n",
        "#from datasets import prepare_data\n",
        "#from model import AirGNN\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='Cora')\n",
        "# parser.add_argument('--seed', type=int, default=15)\n",
        "# parser.add_argument('--model', type=str, default='AirGNN')\n",
        "# parser.add_argument('--alpha', type=float, default=0.1)\n",
        "# parser.add_argument('--lambda_amp', type=float, default=0.1)\n",
        "# parser.add_argument('--lcc', type=str2bool, default=False)\n",
        "# parser.add_argument('--normalize_features', type=str2bool, default=True)\n",
        "# parser.add_argument('--random_splits', type=str2bool, default=False)\n",
        "# parser.add_argument('--runs', type=int, default=10)\n",
        "# parser.add_argument('--epochs', type=int, default=1000)\n",
        "# parser.add_argument('--lr', type=float, default=0.01)\n",
        "# parser.add_argument('--weight_decay', type=float, default=0.0005)\n",
        "# parser.add_argument('--early_stopping', type=int, default=100)\n",
        "# parser.add_argument('--hidden', type=int, default=64)\n",
        "# parser.add_argument('--dropout', type=float, default=0.8, help=\"dropout\")\n",
        "# parser.add_argument('--K', type=int, default=10, help=\"the number of propagagtion in AirGNN\")\n",
        "# parser.add_argument('--model_cache', type=str2bool, default=False)\n",
        "\n",
        "\n",
        "def main():\n",
        "    #args = parser.parse_args()\n",
        "    args = Args()\n",
        "    print('arg : ', args)\n",
        "    dataset, permute_masks = prepare_data(args, lcc=args.lcc)\n",
        "    model = AirGNN(dataset, args)\n",
        "    test_acc = run(dataset, model, args.runs, args.epochs, args.lr, args.weight_decay, args.early_stopping,\n",
        "                   permute_masks, logger=None, args=args) ## TODO: test or val acc\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIuOCAPyIAmz",
        "outputId": "d0cf78cb-ee85-4b1c-c98a-adc22c2cde05"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arg :  <__main__.Args object at 0x7efda3bf0dd0>\n",
            "Data: Data(x=[2708, 1433], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], adj_t=[2708, 2708, nnz=10556])\n",
            "AdaptiveMessagePassing(K=10, alpha=0.1, mode=AirGNN, dropout=0.0, lambda_amp=0.4)\n",
            "device: in train_eval: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [08:00<00:00, 48.08s/run]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.7065, Test Accuracy: 0.844 ± 0.006, Duration: 48.075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  dataset = 'Cora'\n",
        "  seed = 15\n",
        "  model = 'AirGNN'\n",
        "  alpha = 0.1\n",
        "  lambda_amp = 0.4\n",
        "  lcc = False\n",
        "  normalize_features = True\n",
        "  random_splits = False\n",
        "  runs = 10\n",
        "  epochs = 200\n",
        "  lr = 0.01\n",
        "  weight_decay = 0.0005\n",
        "  early_stopping = 100\n",
        "  hidden = 64\n",
        "  dropout = 0.8\n",
        "  K = 10\n",
        "  model_cache = False"
      ],
      "metadata": {
        "id": "LrPv1D7xP-Ft"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Args()"
      ],
      "metadata": {
        "id": "V5BG-YqiNiHP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "F1VriFmQN02B",
        "outputId": "462569f0-6c3a-41dc-8cb0-cbecfa70f349"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cora'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/data"
      ],
      "metadata": {
        "id": "HoqF_-eJN1eM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
