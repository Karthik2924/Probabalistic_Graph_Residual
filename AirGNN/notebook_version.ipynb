{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "63b7jO97FcE-"
      },
      "outputs": [],
      "source": [
        "# !pip install optuna\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "# !pip install torch-geometric\n",
        "# !pip install deeprobust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UH5uPkcNF-Jx"
      },
      "outputs": [],
      "source": [
        "# class Args:\n",
        "#   dataset = 'Cora'\n",
        "#   seed = 15\n",
        "#   model = 'AirGNN'\n",
        "#   alpha = 0.1\n",
        "#   lambda_amp = 0.1\n",
        "#   lcc = False\n",
        "#   normalize_features = True\n",
        "#   random_splits = False\n",
        "#   runs = 10\n",
        "#   epochs = 10\n",
        "#   lr = 0.01\n",
        "#   weight_decay = 0.0005\n",
        "#   early_stopping = 100\n",
        "#   hidden = 64\n",
        "#   dropout = 0.8\n",
        "#   K = 10\n",
        "#   model_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kBsHhz7FQxY",
        "outputId": "f0932ba9-0733-481d-deec-bf8d260f4251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "#adv_attack.py\n",
        "\n",
        "from deeprobust.graph.data import Dataset\n",
        "from deeprobust.graph.defense import GCN\n",
        "from deeprobust.graph.targeted_attack import Nettack\n",
        "import pickle\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "#from datasets import uniqueId, str2bool\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device:\", device)\n",
        "\n",
        "def cache_nettack_feature_attack_data(args):\n",
        "    n_perturbations_candidates = [0, 1, 2, 5, 10, 20, 50, 80]\n",
        "    print(\"====preparing dataset: %s=====\" % (args.dataset))\n",
        "    cache_dataset(args.dataset, n_perturbations_candidates, args)\n",
        "\n",
        "\n",
        "def cache_dataset(dataset_name, n_perturbations_candidates, args):\n",
        "    data = Dataset(root='/tmp/', name=(dataset_name).lower(), seed=args.seed)\n",
        "    adj, features, labels = data.adj, data.features, data.labels\n",
        "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "    surrogate = GCN(nfeat=features.shape[1], nclass=labels.max().item() + 1,\n",
        "                    nhid=16, dropout=0, with_relu=False, with_bias=False, device=device)\n",
        "    surrogate = surrogate.to(device)\n",
        "    surrogate.fit(features, adj, labels, idx_train)\n",
        "\n",
        "    node_list = select_nodes(data)\n",
        "    print(data, node_list, args)\n",
        "\n",
        "    for target_node in tqdm(node_list):\n",
        "        for perturbation in n_perturbations_candidates:\n",
        "\n",
        "            uid = uniqueId(dataset_name, target_node, perturbation)\n",
        "            n_perturbations = int(perturbation)\n",
        "            if n_perturbations != 0:\n",
        "                model = Nettack(surrogate, nnodes=adj.shape[0], attack_structure=False,\n",
        "                                attack_features=True, device=device)\n",
        "                model = model.to(device)\n",
        "                model.attack(features, adj, labels, target_node, n_perturbations, verbose=False)\n",
        "                modified_features = model.modified_features\n",
        "                data.features = modified_features\n",
        "\n",
        "            pickle.dump(data, open(\"./fixed_data/adv_attack/\" + uid + \".pickle\", 'wb'))\n",
        "            print(uid, \"has been save\")\n",
        "\n",
        "def classification_margin(output, true_label):\n",
        "    \"\"\"Calculate classification margin for outputs.\n",
        "    `probs_true_label - probs_best_second_class`\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.Tensor\n",
        "        output vector (1 dimension)\n",
        "    true_label: int\n",
        "        true label for this node\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        classification margin for this node\n",
        "    \"\"\"\n",
        "    probs = torch.exp(output)\n",
        "    probs_true_label = probs[true_label].clone()\n",
        "    probs[true_label] = 0\n",
        "    probs_best_second_class = probs[probs.argmax()]\n",
        "    return (probs_true_label - probs_best_second_class).item()\n",
        "\n",
        "\n",
        "def select_nodes(data, target_gcn=None):\n",
        "    '''\n",
        "    selecting nodes as reported in nettack paper:\n",
        "    (i) the 10 nodes with highest margin of classification, i.e. they are clearly correctly classified,\n",
        "    (ii) the 10 nodes with lowest margin (but still correctly classified) and\n",
        "    (iii) 20 more nodes randomly\n",
        "    '''\n",
        "    adj, features, labels = data.adj, data.features, data.labels\n",
        "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "\n",
        "    if target_gcn is None:\n",
        "        target_gcn = GCN(nfeat=features.shape[1],\n",
        "                         nhid=16,\n",
        "                         nclass=labels.max().item() + 1,\n",
        "                         dropout=0.5, device=device)\n",
        "        target_gcn = target_gcn.to(device)\n",
        "        target_gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
        "    target_gcn.eval()\n",
        "    output = target_gcn.predict()\n",
        "\n",
        "    margin_dict = {}\n",
        "    for idx in idx_test:\n",
        "        margin = classification_margin(output[idx], labels[idx])\n",
        "        if margin < 0:  # only keep the nodes correctly classified\n",
        "            continue\n",
        "        margin_dict[idx] = margin\n",
        "    sorted_margins = sorted(margin_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    high = [x for x, y in sorted_margins[: 10]]\n",
        "    low = [x for x, y in sorted_margins[-10:]]\n",
        "    other = [x for x, y in sorted_margins[10: -10]]\n",
        "    other = np.random.choice(other, 20, replace=False).tolist()\n",
        "\n",
        "    return high + low + other\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     # parser = argparse.ArgumentParser()\n",
        "#     # parser.add_argument('--dataset', type=str, required=True)\n",
        "#     # parser.add_argument('--seed', type=int, default=15)\n",
        "#     # args = parser.parse_args()\n",
        "#     args = Args()\n",
        "#     print('arg : ', args)\n",
        "#     cache_nettack_feature_attack_data(args)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SjTLKod8FZIX"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "import os.path as osp\n",
        "import torch\n",
        "\n",
        "from torch_geometric.datasets import Planetoid, Coauthor, Amazon\n",
        "import torch_geometric.transforms as T\n",
        "from deeprobust.graph.data import Dataset, Dpr2Pyg\n",
        "\n",
        "import argparse\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Unsupported value encountered.')\n",
        "\n",
        "def uniqueId(dataset_name, target_node, perturbation):\n",
        "    return dataset_name.lower() +\"_\"+str(target_node)+\"_\"+str(perturbation)\n",
        "\n",
        "\n",
        "def prepare_data(args, lcc=False):\n",
        "    transform = T.ToSparseTensor()\n",
        "    if args.dataset == \"Cora\" or args.dataset == \"CiteSeer\" or args.dataset == \"PubMed\":\n",
        "        if lcc:\n",
        "            dpr_data = Dataset(root='/tmp/', name=(args.dataset).lower())\n",
        "            dataset = Dpr2Pyg(dpr_data, transform=transform)\n",
        "        else:\n",
        "            dataset = get_planetoid_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_planetoid_splits if args.random_splits else None\n",
        "\n",
        "    elif args.dataset == \"cs\" or args.dataset == \"physics\":\n",
        "        dataset = get_coauthor_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_coauthor_amazon_splits\n",
        "\n",
        "    elif args.dataset == \"computers\" or args.dataset == \"photo\":\n",
        "        dataset = get_amazon_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_coauthor_amazon_splits\n",
        "    print(\"Data:\", dataset[0])\n",
        "\n",
        "    return dataset, permute_masks\n",
        "\n",
        "\n",
        "def get_planetoid_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    dataset = Planetoid(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coauthor_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    dataset = Coauthor(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_amazon_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    dataset = Amazon(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def index_to_mask(index, size):\n",
        "    mask = torch.zeros(size, dtype=torch.bool, device=index.device)\n",
        "    mask[index] = 1\n",
        "    return mask\n",
        "\n",
        "\n",
        "def random_planetoid_splits(data, num_classes, lcc_mask=None):\n",
        "    # Set new random planetoid splits:\n",
        "    # * 20 * num_classes labels for training\n",
        "    # * 500 labels for validation\n",
        "    # * 1000 labels for testing\n",
        "\n",
        "    indices = []\n",
        "    if lcc_mask is not None:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y[lcc_mask] == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0))]\n",
        "            indices.append(index)\n",
        "    else:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0))]\n",
        "            indices.append(index)\n",
        "\n",
        "    train_index = torch.cat([i[:20] for i in indices], dim=0)\n",
        "\n",
        "    rest_index = torch.cat([i[20:] for i in indices], dim=0)\n",
        "    rest_index = rest_index[torch.randperm(rest_index.size(0))]\n",
        "\n",
        "    data.train_mask = index_to_mask(train_index, size=data.num_nodes)\n",
        "    data.val_mask = index_to_mask(rest_index[:500], size=data.num_nodes)\n",
        "    data.test_mask = index_to_mask(rest_index[500:1500], size=data.num_nodes)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def random_coauthor_amazon_splits(data, num_classes, lcc_mask=None, seed=None):\n",
        "    # Set random coauthor/co-purchase splits:\n",
        "    # * 20 * num_classes labels for training\n",
        "    # * 30 * num_classes labels for validation\n",
        "    # rest labels for testing\n",
        "    g = None\n",
        "    if seed is not None:\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "\n",
        "    indices = []\n",
        "    if lcc_mask is not None:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y[lcc_mask] == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0), generator=g)]\n",
        "            indices.append(index)\n",
        "    else:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0), generator=g)]\n",
        "            indices.append(index)\n",
        "\n",
        "    train_index = torch.cat([i[:20] for i in indices], dim=0)\n",
        "    val_index = torch.cat([i[20:50] for i in indices], dim=0)\n",
        "\n",
        "    rest_index = torch.cat([i[50:] for i in indices], dim=0)\n",
        "    rest_index = rest_index[torch.randperm(rest_index.size(0))]\n",
        "\n",
        "    data.train_mask = index_to_mask(train_index, size=data.num_nodes)\n",
        "    data.val_mask = index_to_mask(val_index, size=data.num_nodes)\n",
        "    data.test_mask = index_to_mask(rest_index, size=data.num_nodes)\n",
        "\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vdO8owV9FoLK"
      },
      "outputs": [],
      "source": [
        "#model.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from typing import Optional, Tuple\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "\n",
        "class AirGNN(torch.nn.Module):\n",
        "    def __init__(self, dataset, args):\n",
        "        super(AirGNN, self).__init__()\n",
        "        self.dropout = args.dropout\n",
        "        self.lin1 = Linear(dataset.num_features, args.hidden)\n",
        "        self.lin2 = Linear(args.hidden, dataset.num_classes)\n",
        "        self.prop = AdaptiveMessagePassing(K=args.K, alpha=args.alpha, mode=args.model, args=args)\n",
        "        print(self.prop)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "        self.prop.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.adj_t, data.y\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        x = self.prop(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class AdaptiveMessagePassing(MessagePassing):\n",
        "    _cached_edge_index: Optional[Tuple[Tensor, Tensor]]\n",
        "    _cached_adj_t: Optional[SparseTensor]\n",
        "\n",
        "    def __init__(self,\n",
        "                 K: int,\n",
        "                 alpha: float,\n",
        "                 dropout: float = 0.,\n",
        "                 cached: bool = False,\n",
        "                 add_self_loops: bool = True,\n",
        "                 normalize: bool = True,\n",
        "                 mode: str = None,\n",
        "                 node_num: int = None,\n",
        "                 args=None,\n",
        "                 **kwargs):\n",
        "\n",
        "        super(AdaptiveMessagePassing, self).__init__(aggr='add', **kwargs)\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "        self.mode = mode\n",
        "        self.dropout = dropout\n",
        "        self.cached = cached\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.normalize = normalize\n",
        "        self._cached_edge_index = None\n",
        "        self.node_num = node_num\n",
        "        self.args = args\n",
        "        self._cached_adj_t = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self._cached_edge_index = None\n",
        "        self._cached_adj_t = None\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, edge_weight: OptTensor = None, mode=None) -> Tensor:\n",
        "        if self.normalize:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                raise ValueError('Only support SparseTensor now')\n",
        "\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                cache = self._cached_adj_t\n",
        "                if cache is None:\n",
        "                    edge_index = gcn_norm(  # yapf: disable\n",
        "                        edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                        add_self_loops=self.add_self_loops, dtype=x.dtype)\n",
        "                    if self.cached:\n",
        "                        self._cached_adj_t = edge_index\n",
        "                else:\n",
        "                    edge_index = cache\n",
        "\n",
        "        if mode == None: mode = self.mode\n",
        "\n",
        "        if self.K <= 0:\n",
        "            return x\n",
        "        hh = x\n",
        "\n",
        "        if mode == 'MLP':\n",
        "            return x\n",
        "\n",
        "        elif mode == 'APPNP':\n",
        "            x = self.appnp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K, alpha=self.alpha)\n",
        "\n",
        "        elif mode in ['AirGNN']:\n",
        "            x = self.amp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K)\n",
        "        else:\n",
        "            raise ValueError('wrong propagate mode')\n",
        "        return x\n",
        "\n",
        "    def appnp_forward(self, x, hh, edge_index, K, alpha):\n",
        "        for k in range(K):\n",
        "            x = self.propagate(edge_index, x=x, edge_weight=None, size=None)\n",
        "            x = x * (1 - alpha)\n",
        "            x += alpha * hh\n",
        "        return x\n",
        "\n",
        "\n",
        "    def compute_fast_KL(self, m1, s1, p1, m2, s2, p2):\n",
        "        '''Compute the KLDivergence between two gaussians, KL(P|Q) scaled between 0 and 1'''\n",
        "        #note: precision = p = inv(s)\n",
        "        d = len(m1)\n",
        "        kl = 0.5*( np.log2( np.linalg.det(s2)/np.linalg.det(s1) ) - d + np.trace(np.matmul(p2, s1)) + ((m2-m1).T)@(p2)@(m2-m1) )\n",
        "        scaled_kl = 1 - np.exp(-kl)\n",
        "\n",
        "        return scaled_kl\n",
        "\n",
        "    def amp_forward(self, x, hh, K, edge_index):\n",
        "        lambda_amp = self.args.lambda_amp\n",
        "        gamma = 1 / (2 * (1 - lambda_amp))  ## or simply gamma = 1\n",
        "\n",
        "        for k in range(K):\n",
        "            y = x - gamma * 2 * (1 - lambda_amp) * self.compute_LX(x=x, edge_index=edge_index)  # Equation (9)\n",
        "            gmXin = GaussianMixture(n_components=1, random_state=0).fit(hh.detach().to('cpu').numpy())\n",
        "            gmY = GaussianMixture(n_components=1, random_state=0).fit(y.detach().to('cpu').numpy())\n",
        "            scaled_KL = self.compute_fast_KL(m1=gmXin.means_[0], s1=gmXin.covariances_[0], p1 = gmXin.precisions_[0],\n",
        "                                            m2=gmY.means_[0], s2 = gmY.covariances_[0], p2 = gmY.precisions_[0])\n",
        "            #x = hh + (1-scaled_KL)*(y-hh) #we use 1-kl because we want to more heavily weigh closer samples.\n",
        "            x = hh + (scaled_KL)*(y-hh) #we use 1-kl because we want to more heavily weigh closer samples.\n",
        "\n",
        "            #x = hh + self.proximal_L21(x=y - hh, lambda_=gamma * lambda_amp) # Equation (11) and (12)\n",
        "        return x\n",
        "\n",
        "    def proximal_L21(self, x: Tensor, lambda_):\n",
        "        row_norm = torch.norm(x, p=2, dim=1)\n",
        "        score = torch.clamp(row_norm - lambda_, min=0)\n",
        "        index = torch.where(row_norm > 0)             #  Deal with the case when the row_norm is 0\n",
        "        score[index] = score[index] / row_norm[index] # score is the adaptive score in Equation (14)\n",
        "        return score.unsqueeze(1) * x\n",
        "\n",
        "    def compute_LX(self, x, edge_index, edge_weight=None):\n",
        "        x = x - self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(K={}, alpha={}, mode={}, dropout={}, lambda_amp={})'.format(self.__class__.__name__, self.K,\n",
        "                                                               self.alpha, self.mode, self.dropout,\n",
        "                                                               self.args.lambda_amp)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqyrNrcBG81q",
        "outputId": "e77eaca4-33e1-4530-9719-fcfd15f127ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arg :  <__main__.Args object at 0x0000016D573B5CD0>\n",
            "device: in train_eval: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "#from datasets import prepare_data\n",
        "#from model import AirGNN\n",
        "from tqdm import tqdm\n",
        "#from train_eval import evaluate\n",
        "import numpy as np\n",
        "from torch import tensor\n",
        "#from datasets import uniqueId, str2bool\n",
        "import torch_geometric.transforms as T\n",
        "from deeprobust.graph.data import Dpr2Pyg\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='Cora')\n",
        "# parser.add_argument('--model', type=str, default='AirGNN')\n",
        "# parser.add_argument('--normalize_features', type=str2bool, default=True)\n",
        "# parser.add_argument('--random_splits', type=str2bool, default=False)\n",
        "# parser.add_argument('--runs', type=int, default=10)\n",
        "# parser.add_argument('--dropout', type=float, default=0.5, help=\"dropout\")\n",
        "# parser.add_argument('--hidden', type=int, default=64)\n",
        "# parser.add_argument('--K', type=int, default=10, help=\"the number of propagagtion in AirGNN\")\n",
        "# parser.add_argument('--alpha', type=float, default=None)\n",
        "# parser.add_argument('--lambda_amp', type=float, default=0.1)\n",
        "\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = Args()\n",
        "\n",
        "print('arg : ', args)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device: in train_eval:\", device)\n",
        "\n",
        "\n",
        "def main():\n",
        "    acc_lst_dic = {}\n",
        "    final_result = {}\n",
        "    n_perturbations_candidates = [0, 1, 2, 5, 10, 20, 50, 80]\n",
        "    for perturbation_number in n_perturbations_candidates:\n",
        "        acc_lst_dic[perturbation_number] = []\n",
        "\n",
        "    if args.dataset in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
        "        for run_k in range(args.runs):\n",
        "            perturbation_acc_dic = nettack_run(args.dataset.lower(), run_k, n_perturbations_candidates)\n",
        "            for key, val in perturbation_acc_dic.items():\n",
        "                acc_lst_dic[key].append(val)\n",
        "\n",
        "        for key, val in acc_lst_dic.items():\n",
        "            acc_lst = tensor(val)\n",
        "            final_result[key] = '{:.3f} Â± {:.3f}'.format(acc_lst.mean().item(), acc_lst.std().item())\n",
        "            \n",
        "        print(\"Dataset:{}, model:{}\".format(args.dataset, args.model))\n",
        "        print(\"Average performance on 40 targeted nodes with 10 runs:\", final_result)\n",
        "\n",
        "def nettack_run(dataset_name, run_k, n_perturbations_candidates):\n",
        "    node_list = get_target_nodelst(dataset_name)\n",
        "    num = len(node_list)\n",
        "    assert num == 40\n",
        "\n",
        "    target_accuracy_dic = {}\n",
        "    target_accuracy_summary_dic = {}\n",
        "    for key in n_perturbations_candidates:\n",
        "        target_accuracy_dic[key] = 0\n",
        "        target_accuracy_summary_dic[key] = []\n",
        "\n",
        "    for target_node in node_list:\n",
        "        for perturbation in n_perturbations_candidates:\n",
        "            n_perturbations = int(perturbation)\n",
        "            uid = uniqueId(dataset_name, target_node, perturbation)\n",
        "            data = get_adv_data(uid)\n",
        "            target_node_acc = adv_test(target_node, data, data.adj[target_node].nonzero()[1].tolist(), run_k)\n",
        "            if target_node_acc == 0:\n",
        "                target_accuracy_dic[perturbation] += 1\n",
        "\n",
        "            print(\"=========Attacked Node: {:d}, n_perturbations: {:.2f}=========\".format(target_node, perturbation))\n",
        "        print(args.model, args.lambda_amp)\n",
        "\n",
        "    assert num == 40\n",
        "    for key in target_accuracy_dic.keys():\n",
        "        target_accuracy_dic[key] = 1 - target_accuracy_dic[key] / num\n",
        "\n",
        "    print(\"Accuracy on 40 target nodes:\", target_accuracy_dic)\n",
        "    return target_accuracy_dic\n",
        "\n",
        "\n",
        "def get_target_nodelst(dataset_name):\n",
        "    allfiles = os.listdir(\"./fixed_data/adv_attack\")\n",
        "    remain = [single_file for single_file in allfiles if dataset_name.lower() in single_file]\n",
        "    node_indexes = []\n",
        "    for i in remain:\n",
        "        if len(i.split(\"_\"))  == 3:\n",
        "            node_indexes.append(int(i.split(\"_\")[1]))\n",
        "\n",
        "    node_list = list(set(node_indexes))\n",
        "    assert len(node_list) == 40\n",
        "    return node_list\n",
        "\n",
        "def get_adv_data(uid):\n",
        "    print(\"./fixed_data/adv_attack/\"+uid+\".pickle\")\n",
        "    if os.path.isfile(\"./fixed_data/adv_attack/\"+uid+\".pickle\"):\n",
        "        return pickle.load(open(\"./fixed_data/adv_attack/\"+uid+\".pickle\",'rb'))\n",
        "    else:\n",
        "        raise Exception(\"ERROR\" + uid + \" file not found\")\n",
        "\n",
        "def adv_test(key_node_index, attacked_dpr_data, neighbor_lst, run_k):\n",
        "    transform = T.ToSparseTensor()\n",
        "    dataset = Dpr2Pyg(attacked_dpr_data, transform=transform)\n",
        "    data = dataset[0]\n",
        "    data = data.to(device)\n",
        "\n",
        "    if args.model in [\"APPNP\", \"AirGNN\", \"MLP\"]:\n",
        "        model = AirGNN(dataset, args)\n",
        "    else:\n",
        "        raise Exception(\"Unsupported model mode!!!\")\n",
        "\n",
        "    # 10 best models will be tested on the same attacked data\n",
        "    model.to(device).reset_parameters()\n",
        "    checkpointPath = \"./model/lcc/{}_{}_best_model_run_{}.pth\".format(args.dataset, args.model, run_k)\n",
        "    print(\"checkpointPath:\", checkpointPath)\n",
        "    checkpoint = torch.load(checkpointPath)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # test the prediction of targeted node\n",
        "    model.eval()\n",
        "    logits = model(data)\n",
        "    probs = torch.exp(logits[[key_node_index]])\n",
        "    target_node_acc = (logits.argmax(1)[key_node_index] == data.y[key_node_index]).item()  # True/False\n",
        "    # print(\"single_target_predict\\n:\", logits.argmax(1)[key_node_index].item(), data.y[key_node_index].item(), target_node_acc)\n",
        "    return target_node_acc\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r5cu2jl8HrmQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIuOCAPyIAmz",
        "outputId": "d0cf78cb-ee85-4b1c-c98a-adc22c2cde05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arg :  <__main__.Args object at 0x0000016D09676580>\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\GraphNeuralNetworks\\AirGNN\\notebook_version.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m test_acc\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     main()\n",
            "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\GraphNeuralNetworks\\AirGNN\\notebook_version.ipynb Cell 8\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m args \u001b[39m=\u001b[39m Args()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39marg : \u001b[39m\u001b[39m'\u001b[39m, args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m dataset, permute_masks \u001b[39m=\u001b[39m prepare_data(args, lcc\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mlcc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m model \u001b[39m=\u001b[39m AirGNN(dataset, args)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m test_acc \u001b[39m=\u001b[39m run(dataset, model, args\u001b[39m.\u001b[39mruns, args\u001b[39m.\u001b[39mepochs, args\u001b[39m.\u001b[39mlr, args\u001b[39m.\u001b[39mweight_decay, args\u001b[39m.\u001b[39mearly_stopping,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                permute_masks, logger\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, args\u001b[39m=\u001b[39margs) \u001b[39m## TODO: test or val acc\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\GraphNeuralNetworks\\AirGNN\\notebook_version.ipynb Cell 8\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(args, lcc)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         dataset \u001b[39m=\u001b[39m Dpr2Pyg(dpr_data, transform\u001b[39m=\u001b[39mtransform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         dataset \u001b[39m=\u001b[39m get_planetoid_dataset(args\u001b[39m.\u001b[39;49mdataset, args\u001b[39m.\u001b[39;49mnormalize_features, transform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     permute_masks \u001b[39m=\u001b[39m random_planetoid_splits \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mrandom_splits \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mdataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m args\u001b[39m.\u001b[39mdataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mphysics\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\GraphNeuralNetworks\\AirGNN\\notebook_version.ipynb Cell 8\u001b[0m in \u001b[0;36mget_planetoid_dataset\u001b[1;34m(name, normalize_features, transform)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_planetoid_dataset\u001b[39m(name, normalize_features\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(osp\u001b[39m.\u001b[39mdirname(osp\u001b[39m.\u001b[39mrealpath(\u001b[39m__file__\u001b[39;49m)), \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     dataset \u001b[39m=\u001b[39m Planetoid(path, name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/GraphNeuralNetworks/AirGNN/notebook_version.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mif\u001b[39;00m transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m normalize_features:\n",
            "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import optuna\n",
        "import torch_geometric.transforms as T\n",
        "import argparse\n",
        "#from datasets import str2bool\n",
        "#from train_eval import run\n",
        "#from datasets import prepare_data\n",
        "#from model import AirGNN\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='Cora')\n",
        "# parser.add_argument('--seed', type=int, default=15)\n",
        "# parser.add_argument('--model', type=str, default='AirGNN')\n",
        "# parser.add_argument('--alpha', type=float, default=0.1)\n",
        "# parser.add_argument('--lambda_amp', type=float, default=0.1)\n",
        "# parser.add_argument('--lcc', type=str2bool, default=False)\n",
        "# parser.add_argument('--normalize_features', type=str2bool, default=True)\n",
        "# parser.add_argument('--random_splits', type=str2bool, default=False)\n",
        "# parser.add_argument('--runs', type=int, default=10)\n",
        "# parser.add_argument('--epochs', type=int, default=1000)\n",
        "# parser.add_argument('--lr', type=float, default=0.01)\n",
        "# parser.add_argument('--weight_decay', type=float, default=0.0005)\n",
        "# parser.add_argument('--early_stopping', type=int, default=100)\n",
        "# parser.add_argument('--hidden', type=int, default=64)\n",
        "# parser.add_argument('--dropout', type=float, default=0.8, help=\"dropout\")\n",
        "# parser.add_argument('--K', type=int, default=10, help=\"the number of propagagtion in AirGNN\")\n",
        "# parser.add_argument('--model_cache', type=str2bool, default=False)\n",
        "\n",
        "\n",
        "def main():\n",
        "    #args = parser.parse_args()\n",
        "    args = Args()\n",
        "    print('arg : ', args)\n",
        "    dataset, permute_masks = prepare_data(args, lcc=args.lcc)\n",
        "    model = AirGNN(dataset, args)\n",
        "    test_acc = run(dataset, model, args.runs, args.epochs, args.lr, args.weight_decay, args.early_stopping,\n",
        "                   permute_masks, logger=None, args=args) ## TODO: test or val acc\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LrPv1D7xP-Ft"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  dataset = 'Cora'\n",
        "  seed = 15\n",
        "  model = 'AirGNN'\n",
        "  alpha = 0.1\n",
        "  lambda_amp = 0.4\n",
        "  lcc = False\n",
        "  normalize_features = True\n",
        "  random_splits = False\n",
        "  runs = 10\n",
        "  epochs = 200\n",
        "  lr = 0.01\n",
        "  weight_decay = 0.0005\n",
        "  early_stopping = 100\n",
        "  hidden = 64\n",
        "  dropout = 0.8\n",
        "  K = 10\n",
        "  model_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V5BG-YqiNiHP"
      },
      "outputs": [],
      "source": [
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "F1VriFmQN02B",
        "outputId": "462569f0-6c3a-41dc-8cb0-cbecfa70f349"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Cora'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args.dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoqF_-eJN1eM"
      },
      "outputs": [],
      "source": [
        "/content/data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMnBr6sqZy49h/GS88Z5NGu",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
